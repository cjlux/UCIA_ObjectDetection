\documentclass[10pt,serif,mathserif,compress,hyperref={colorlinks}]{beamer}
\mode<presentation>
\usepackage{pgf}
\usepackage{pgfpages}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{lmodern}
\usepackage{lastpage}
\usepackage{comment}
\usepackage{geometry}
\usepackage[most]{tcolorbox}
\tcbuselibrary{skins}
\usepackage{beamerthemesplit}
\usepackage{amsmath, amsfonts, epsfig, xspace}
\usepackage{pstricks,pst-node}
\usepackage{multimedia}
\usepackage{pifont}   % zapf dingbats
\usepackage{marvosym} % MarVoSym dingbats
\usepackage{wasysym}
%\usepackage{animate}

\usepackage{graphicx}% for including figures
\usepackage{tikz}
\usepackage[tikz]{bclogo}
\usetikzlibrary{positioning,decorations.pathreplacing,arrows}
\usepackage{tikzsymbols}

\setlength{\parindent}{0pt}

\input{colors}
\input{commands}

%Beamer theme [\usetheme{jlcKeynote}]
\renewcommand\sfdefault{phv}
\renewcommand\familydefault{\sfdefault}
\usetheme{default}
\usepackage{color}
\useoutertheme{default}
\usepackage{texnansi}
\usepackage{marvosym}
\definecolor{bottomcolour}{rgb}{0.32,0.3,0.38}
\definecolor{middlecolour}{rgb}{0.08,0.08,0.16}
\setbeamerfont{title}{size=\Huge}
\setbeamercolor{structure}{fg=gray!50!white}
\setbeamertemplate{frametitle}[default]%[center]
%\setbeamercolor{normal text}{bg=black, fg=white}
\setbeamertemplate{items}[circle]
\setbeamerfont{frametitle}{size=\Large}
\setbeamertemplate{navigation symbols}{} %no nav symbols
% Beamer theme

\useoutertheme[subsection=false]{miniframes}
\setbeamercolor{background canvas}{bg=gray!50!white}
\setbeamercolor{structure}{bg=white, fg=gray}
\setbeamercolor{frametitle}{fg=DarkChocolate, bg=gray!50}
\setbeamertemplate{itemize item}{\small\gray{$\CIRCLE$}}
\setbeamertemplate{itemize subitem}{\tiny\gray{$\CIRCLE$}}
\settowidth{\leftmargini}{\usebeamertemplate{itemize item}}
\addtolength{\leftmargini}{\labelsep}

\setbeamercovered{transparent}

\hypersetup{linkcolor=Yellow}
\hypersetup{citecolor=DeepPink4}
\hypersetup{urlcolor=DarkBlue}
\hypersetup{anchorcolor=Magenta}

\vspace*{1mm}
\title[\hspace*{.8\linewidth}\insertframenumber/\inserttotalframenumber]
{\fontsize{17}{17}\selectfont{\textbf{Applications \& Challenges of AI\\in Space Missions}\\[2mm]{A quick review}}\\[5mm]
 \fontsize{12}{12}\selectfont{@ENSAM Semaine d'approfondissement}}

\subtitle{}

\author[{\tiny JLC -- MAY. 2024 -- V1.0}\hspace*{.8\linewidth}]
       {\fontsize{10}{10}\selectfont{Jean-Luc.Charles\,@\,mailo.com}\\[2mm]
       \includegraphics[height=1.5cm]{images/logo-couleur-rvb-en.png}}

\institute{}

\date{\small May 2024}
\titlegraphic{
  \href{https://creativecommons.org/licenses/by-sa/4.0/}
       {\includegraphics[height=5mm]{images/CC-BY-SA.jpeg}}     
}

\logo{}

%\tcbset{enhanced, boxrule=0.2pt, sharp corners, drop lifted shadow=black,
%  colback=Chocolate!25!white,colframe=Chocolate!75!black, fonttitle=\Large}

\tcbset{enhanced, boxrule=0.2pt, sharp corners, drop lifted shadow,
    width=1.0\textwidth, left=5pt, left skip=-5pt,
    colback=Chocolate!25!white,colframe=Chocolate!75!black}

\begin{document}

\renewcommand{\ttdefault}[0]{lmtt}
\newcommand{\boldtt}[1]{{\ttfamily\bfseries #1}}

%====== # 1 ==================================================
\frame[plain]{\titlepage}
%=============================================================

\setbeamercolor{structure}{fg=gray!50!white}

%\section{Welcome}

%====== # 2 ==================================================
\begin{frame}{}
  
  %\vspace*{-3mm}%
  \begin {bclogo}[noborder=true, couleur=gray!50, couleurBarre=Chocolate, logo=\bctrombone, marge=0, margeG=-0.5]
    {\ A quick review to get informed with...}
    \medskip
    \begin{itemize}
    \item Historical benchmarks regarding AI
    \item Machine Learning (ML) : the most visible of AI fields
    \item Main definitions \& concepts of ML
    \item Current state-of-the-art of ML applications
    \item Specific applications \& challenges of AI (ML) in Space Mission.
    \end{itemize}
  \end{bclogo}
  
  \visible<2->{
    \begin {bclogo}[noborder=true, couleur=gray!50, couleurBarre=Chocolate, logo=\bctrombone, margeG=-0.8]
      {\ My profile}
      \medskip
      \begin{itemize}
      \item I taught Python programming (Scientific \& Object Oriented) for years at ENSAM
      \item I started to get interested in Machine Learning (ML) in 2015
      \item I wrote several materials in ML : workshop, project \& practical work
        for different schools (ENSAM, ENSEIRB, ENSPIMA, PPU...)
      \end{itemize}
    \end{bclogo}
  }
  
\end{frame}
%=============================================================

\section{The historical way}

\subsection{The historical way...}

%====== # 3 ==================================================
\begin{frame}{AI : the historical way... from 1950 to 2000s}
  \hspace*{-8mm}\includegraphics[width=1.15\textwidth]{images/Brief History of NN - Kate Strachnyi.png}\\
  \center{\footnotesize Source: \href{https://medium.com/analytics-vidhya/brief-history-of-neural-networks-44c2bf72eec}
    {Kate Strachni: "Brief History of Neural Netowrks", medium.com}}

\end{frame}
%=============================================================

%====== # 4 ==================================================
\begin{frame}{The historical way... the turning point of the 2000s}
  \hspace*{-8mm}\includegraphics[width=1.15\textwidth]{images/A brief History of NN - Pumalin-2.png}\\
  \center{\footnotesize Source: \href{https://pub.towardsai.net/a-brief-history-of-neural-nets-472107bc2c9c}
    {Pumalin: "A Brief History of Neural Nets", medium.com}}

\end{frame}
%=============================================================

%====== # 5 ==================================================
\begin{frame}{The historical way... post 2012s, the acceleration}
  \hspace*{-8mm}\includegraphics[width=1.1\textwidth]{images/Ten Years of AI in Review-2.png}\\
  \center{\footnotesize Source: \href{https://towardsdatascience.com/ten-years-of-ai-in-review-85decdb2a540}
    {Thomas A Dorfe: "Ten Years of AI in Review", medium.com}}
  
\end{frame}
%=============================================================

%====== # 6 ==================================================
\begin{frame}{The historical way... post 2023s, crazy acceleration}
    \hspace*{-12mm}\includegraphics[width=.75\textwidth]{images/Ten Years of AI in Review-2.png}
    \begin{minipage}{\textwidth}
      \vspace*{-59.5mm}\hspace*{58mm}\frame{\includegraphics[width=0.51\textwidth]{images/post_2023-LLMs-Bots-2.png}}
    \end{minipage}\\
  \center{\footnotesize Source: \href{https://towardsdatascience.com/ten-years-of-ai-in-review-85decdb2a540}
    {Thomas A Dorfe: "Ten Years of AI in Review", medium.com}}
\end{frame}
%=============================================================

\section{AI Definitions}

%====== # 7 ==================================================
\begin{frame}{Artificial Intelligence ?}

  \vspace*{-5mm}
  \begin {bclogo}[noborder=true, couleur=gray!50, couleurBarre=Chocolate, logo=\bctrombone, margeG=-0.8]
    {}
    \vspace*{-5mm}
    Historically\footnote{{\tiny first used in 1956 by \href{https://en.wikipedia.org/wiki/John\_McCarthy\_\%28computer\_scientist\%29}{John McCarthy},
    researcher at Stanford during the Dartmouth conference}} {\it badly chosen} term! Ambiguous current meaning...\\
    Many (contradictory) definitions depending on periods and authors...
    \end{bclogo}

  \visible<2->{

  \begin{itemize}
    {%\small
    \item {\em ''...the science of making computers do things that require intelligence when done by humans.}''
      {\tiny \href{http://www.alanturing.net/turing\_archive/pages/reference\%20articles/what\%20is\%20ai.html}{Alan Turing, 1940}}\smallskip
      
    \item {\em ''the field of study that gives computers the ability to learn without being explicitly programmed.''}
      {\tiny  \href{http://infolab.stanford.edu/pub/voy/museum/samuel.html}{Arthur Samuel, 1960}}\smallskip
      
    \item {\em ''A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P,
      if its performance at tasks in T, as measured by P, improves with experience E.''}
      {\tiny \href{https://www.cs.cmu.edu/~tom/}{Tom Mitchell, 1997}}\smallskip
      
    \item Notion of {\em intelligent agent}, {\em rational agent} \\
      {\em ''...agent that acts in such a way as to reach the best solution or, in an uncertain environment, the best predictable solution.}''\\[-2mm]
      {\tiny  \href{http://aima.cs.berkeley.edu/translations.html}{Stuart Russel, Peter Norvig, ``Intelligence Artificielle'' 2015}}
    }
  \end{itemize}
  }
  
\end{frame}
%=============================================================

%====== # 8 ==================================================
\begin{frame}{Artificial Intelligences ?}

  \textbf{Symbolic AI}
  \visible<2->{%
    \begin{itemize}
    \item Build systems that think exactly the same way that people do with the ability to reason in general.
    \end{itemize}
  }
  \smallskip
  \textbf{Strong / General AI }
  \visible<3->{%
    \begin{itemize}
    \item Build systems that think exactly the same way that people do with the ability to reason in general.
    \item Try also to explain how humans think... \Chocolate{Whe are not yet here}.
    \end{itemize}
  }
  \smallskip
  \textbf{Weak / Narrow AI}
  \visible<4->{% 
    \begin{itemize}
    \item Build systems that can {\bf behave like humans}.
    \item AI systems designed for specific tasks.
    \item The results will tell us nothing about how humans think.
    \item \Chocolate{We already are there}... We use it every day!\\
      (anti-spam, facial/voice recognition, language translation...)
    \end{itemize}
  }
  \smallskip
  \textbf{Multimodal AI}
  \visible<5->{%
    \begin{itemize}
    \item AI systems designed to react to multiple inputs\\
      (texte, graphic, sound...).
    \end{itemize}
  }
\end{frame}
%=============================================================

\section{ML definiton \& concepts}

%====== # 9 ==================================================
\begin{frame}{Machine Learning: a field of AI}

  {\small Page from \href{https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12}
  {medium.com/machine-learning-for-humans/...}}\\[2mm]
  
  \hspace*{-10mm}\includegraphics[width=1.2\textwidth]{images/AI-from_MachineLearningForHumans.png}
  \vspace*{-8mm}
  
\end{frame}
%=============================================================

%====== #10 ==================================================
\begin{frame}{Branches of Machine Learning}

  %% Excellent: https://www.ibm.com/cloud/learn/machine-learning?lnk=fle

  \begin{tcolorbox}[title={\bf Supervised learning}]
    {\bf labeled dataset} is used to train algorithms:
    \begin{itemize}
    \item \textbf{Classification}
      \begin{itemize}
      \item Images classification
      \item Objects detection in images
      \item Speech recognition...
      \end{itemize}
    \item \textbf{Regression}
      \begin{itemize}
      \item Predict a value...
      \end{itemize}
    \item \textbf{Anomaly detection}
      %% anomaliy detection with supervised laerning suppose that there are no "new annomaly" in the
      %% datat set to process, because the anomalies have been learned and the algorithm will not
      %% recognize new anomaly that was not learned...
      \begin{itemize}
      \item Spam detection
      \item Manufacturing: finding known (learned) defects
      \item Weather prediction
      \item Diseases classification...
      \end{itemize}        
    \end{itemize}
    \vspace*{-1mm}$\cdots$
  \end{tcolorbox}
  
\end{frame}
%=============================================================

%====== #11 ==================================================
\begin{frame}{Branches of Machine Learning}
  \begin{tcolorbox}[title={\bf Unsupervised learning}]
    Analyze and cluster \textbf{unlabeled datasets}:
    \begin{itemize}
    \item \textbf{Clustering} \& \textbf{Grouping} 
      \begin{itemize}
      \item Data mining, web data grouping, news grouping...
      \item Market segmentation
      \item Astronomical data analysis...
      \end{itemize}        
    \item \textbf{Anomaly Detection}
      \begin{itemize}
      \item Fraud detecion
      \item Manufacturing: finding defects even new ones
      \item Monitoring abnormal activity: failure, hacker, fraud...
      \item Fake account on Internet...
      \end{itemize}
    \item \textbf{Dimensionality reduction}
      \begin{itemize}
      \item Compress data using fewer numbers...
      \end{itemize}
    \end{itemize}
    \vspace*{-1mm}$\cdots$
  \end{tcolorbox}    
\end{frame}
%=============================================================

%====== #12 ==================================================
\begin{frame}{Branches of Machine Learning}  
  \begin{tcolorbox}[title={\bf Deep Reinforcement Learning} DRL]
    An agent (the Neural Network) learns how to drive an environment by maximising a \textbf{reward}:
    \begin{itemize}

    \item \textbf{Control/command}
      \begin{itemize}
      \item Controlling \href{run:./videos/trained-PPO-example.webm}{robots}, drones, \href{run:./videos/DRL-Cartpol.webm}{mecatronic systems}
      \item Factory optimization
      \item Financial (stock) trading...
      \end{itemize}        
    \item \textbf{Decision making}
      \begin{itemize}
      \item games (video games)
      \item financial analysis...
      \end{itemize}
    \end{itemize}
    \vspace*{-1mm}$\cdots$
  \end{tcolorbox}

  \begin{minipage}{\textwidth}
    \vspace*{-28mm}\hspace*{50mm}\includegraphics[width=.6\textwidth]{images/DRL.png}
  \end{minipage}

  \vfill
\end{frame}
%=============================================================

\subsection{ML algorithms}

%====== #13 ==================================================
\begin{frame}{}

  %See \href{https://www.ibm.com/cloud/learn/machine-learning}{www.ibm.com/cloud/learn/machine-learning}
  
  \begin{tcolorbox}[title=Various approaches for ML algorithms]
    {\small
      \begin{minipage}[t]{.55\textwidth}
        \bfdarkchoco{Supervised learning:}
        \begin{itemize}
        \item<1-> \only<1>{\Blue{Neural Networks}}\only<2>{\Blue{Neural Networks}}
        \item<1> Bayesian inference
        \item<1> Random forest
        \item<1> Decision Tree
        \item<1> Support Vector Machine (SVM)
        \item<1> K-Nearest Neighbor
        \item<1> Linear regression
        \item<1> Logistic regression
        \item<1>...
        \end{itemize}
      \end{minipage}\begin{minipage}[t]{.55\textwidth}
        \bfdarkchoco{Unsupervised learning:}
        \begin{itemize}
        \item<1-> \only<1>{\Blue{Neural Networks}}\only<2>{\Blue{Neural Networks}}
        \item<1> Principal Composant Analysis
        \item<1> Singular Value Decomposition
        \item<1> K-mean \& Prob. clustering
        \item<1>...
        \end{itemize}
        \medskip
        \bfdarkchoco{Reinforcement learning:}
        \begin{itemize}
        \item<1-> \only<1>{\Blue{Neural Networks} (Q-learning, Actor-Critic, DDPG, PPO...)}\only<2>{\Blue{Neural Networks}\\[5mm]} 
        \item<1> Monte Carlo
        \item<1> SARSA
        \item<1>...
        \end{itemize}
        
      \end{minipage}
    }
  \end{tcolorbox}    
  \visible<2->{A large variety of domains $\leadsto$ success of \Blue{\bf Artificial Neural Networks}}
\end{frame}
%=============================================================

\subsection{Neural Networks achitectures}

%Physics-Informed Machine Learning Models

%====== #14 ==================================================
\begin{frame}{Neural Network Architectures}

Many NN architectures for many applications (non-exhaustive list):
\begin{itemize}
  \item<1-> \bfdarkchoco{Feed Forward}: the simplest architecture made of successive layers of neurones, with {\em Feed Forward} and {\em Back Propagation} algorithms.
  \item<2-> \bfdarkchoco{Convolutional} (CNN): Mostly used for analyzing and classifying images.
  \item<3-> \bfdarkchoco{Recurrent} (RNN): Used for time series, like the Long Short-Term Memory (LSTM) algorithm.
  \item<4-> \bfdarkchoco{Transformers} : Recently used for Natural Language Processing and then for image classification.
  \item<5-> \bfdarkchoco{Auto Encoder} (AEN): Dimensionality reduction, Feature extraction, Denoising of data/images, Inputing missing data.
  \item<6-> \bfdarkchoco{Generative Adversarial} (GAN): to generate text, images, music...
  \item<7-> \bfdarkchoco{Large Language Model} (LLM): read text, sound, write books, images, speak, make music ...ChatGPT
\end{itemize}
{\footnotesize\center[Synthetic Graphical chart: \href{https://chart-studio.plotly.com/~SolClover/90.embed?autosize=true&referrer=https\%3A\%2F\%2Ftowardsdatascience.com\%2F}{from Saul Dobilas on Medium}] }
\end{frame}
%=============================================================

\section{ML Applications}

%====== #15 ==================================================
\begin{frame}{Fields \& applications of ML}

  \begin{tcolorbox}[height=6cm, add to width=.7cm, title={\em Computer Vision}: some applications...]
    \begin{minipage}[t][][t]{.6\textwidth}
      \begin{itemize}
      \item<1-> Image Classification
      \item<2-> Object Detection 
      \item<3-> (Semantic) Segmentation
      \item<4-> Image Generation\\
        {\tiny \href{https://www.leptidigital.fr/productivite/meilleurs-generateurs-images-ia-30857/}{Les 23 meilleurs générateurs d’images par IA (Gratuits et Pros)}}
      \item<5-> Pose Estimation
      \item<6-> Style transfer
      \item<7-> OCR ({\small Optical Character Recognition})
      \item<7-> ...
      \end{itemize}
    \end{minipage}\begin{minipage}[t][][b]{.4\textwidth}
      \only<1>{\includegraphics[width=1.\textwidth]{images/image_classification_sunflowers.png}}
      \only<2>{\includegraphics[width=1.\textwidth]{images/object_detection_aple-banana.png}\\[-2mm]
        {\centerline{\tiny Image credit: \href{https://www.tensorflow.org/lite/examples/object_detection/overview}{Tensorflow}}}}
      \only<3>{\hspace*{-20mm}\includegraphics[width=1.5\textwidth]{images/Semantic-segmentation.png}\\[-2mm]
        {\centerline{\tiny Image credit: \href{https://blog.tensorflow.org/2019/11/updated-bodypix-2.html}{Tensorflow}}}}
      \only<4>{\includegraphics[width=1.\textwidth]{images/image_generation.png}\\[-2mm]
        {\centerline{\tiny Image credit: \href{https://github.com/NVlabs/stylegan}{stylegan}}}}
      \only<5>{\includegraphics[width=1.\textwidth]{images/pose_estimation_TF.png}\\[-2mm]
        {\centerline{\tiny Image credit: \href{https://www.tensorflow.org/lite/examples/pose_estimation/overview}{Tensorflow}}}}
      \only<6>{\vspace*{5mm}\hspace*{-30mm}\includegraphics[width=1.7\textwidth]{images/style_transfer.png}\\[-2mm]
        {\centerline{\tiny Image credit: \href{https://www.tensorflow.org/lite/examples/style_transfer/overview}{Tensorflow}}}}
      \only<7>{\includegraphics[width=1.\textwidth]{images/east-out-stop2.jpg}\\[-2mm]
        {\centerline{\tiny Image credit: \href{https://www.tensorflow.org/lite/examples/optical_character_recognition/overview}{Tensorflow}}}}
    \end{minipage}
  \end{tcolorbox}
  
\end{frame}
%=============================================================

%====== #16 ==================================================
\begin{frame}{Fields \& applications of ML}

  \begin{tcolorbox}[title={\em NLP({\small Natural Language Processing})}: some applications... ]
    \begin{itemize}
    \item<1-> Natural Language Understanding (NLU) 
    \item<1-> Natural Language Generation (NLG)
    \item<1-> Speech recognition / Speech Synthesis (Text To Speech)
    \item<1-> Machine Translation (languages)
    \item<1-> Virtual agents and ChatBots
    \item<1-> LLM ChatBots (Agent Conversationnel)
    \item<1-> Optical character recognition (OCR)
    \item<1-> ...
    \end{itemize}
  \end{tcolorbox}   
  
\end{frame}
%=============================================================

\subsection{Explainability}

%====== #17 ==================================================
\begin{frame}{Societal issues: {\bf Explainabilty}}

  \DarkGreen{\bf Explainability} is fast becoming a top priority in research, where it is often
  abbreviated as \\
  \hspace*{10mm}{\bf xAI} \hspace*{10mm}{\bf Explainable} Artificial Intelligence\\
  \hspace*{10mm}{\bf iML} \hspace*{10mm}{\bf Interpretable} Machine learning
  \bigskip

  \begin{tcolorbox}[title={\bf Explainability}]
    \begin{itemize}
    \item {\bf Unexplainability} of the results computed by Neural Networks still constitutes an obstacle to their dissemination today.
    \item Deep learning with Neural Networks is often denigrated as a {\bf Black box} by scientists with a Cartesian approach.
    \item Even ML developpers have difficulties to explain by which way a NN decision has been computed
    \end{itemize}
  \end{tcolorbox}   
    
  
\end{frame}
%=============================================================

\subsection{decision-making}

%====== #18 ==================================================
\begin{frame}{Societal issues: {\bf Decision-making, Certification}}
  
  \begin{tcolorbox}[title={\bf Decision-making}]
    \begin{itemize}
    \item An increasing amount of {\bf decisions} in sensible fields (criminal justice, health, insurance, defense...) are being ceded to ML algorithms to the detriment of
  human control, raising concern for loss of fairness and equitability.
    \item Decision-making algorithms rely inevitably on assumptions (such as the quality of data the algorithm is trained with)
    \end{itemize}
  \end{tcolorbox}   

  \visible<2>{
  \begin{tcolorbox}[title={\bf Evalation, Certification}]
    \begin{itemize}      
    \item The evaluation and the certification of AI systems is still a major issue for their integration in the industry
    \item Formal certification of ML algorithms is still a subject of research today
    \item Example:
      \href{https://www.lne.fr/fr/service/certification/certification-processus-ia}
           {LNE: Process certification for AI},\\
      \href{https://www.hhi.fraunhofer.de/en/departments/ai/technologies-and-solutions/auditing-and-certification-of-ai-systems.html}
           {Fraunhoher: Auditing and Certification of AI Systems}
    \end{itemize}
  \end{tcolorbox}
  }
    
\end{frame}
%=============================================================

\section{AI in Space Missions}

%====== #18 ==================================================
\begin{frame}{Applications \& Challenges of AI in Space Missions}

%ESA video : https://www.esa.int/Enabling_Support/Preparing_for_the_Future/Discovery_and_Preparation/Artificial_intelligence_in_space

  All the applications of IA/ML already described exist also in space mission context:
  \begin{itemize}
    \item Computer Vision
    \item (Big) Data analysis
    \item Autonomous systems
    \item Intelligent Moving Robots...
  \end{itemize}
    
  %Using AI to mesure the size of icebergs: https://www.esa.int/ESA_Multimedia/Images/2023/11/Using_AI_to_measure_the_size_of_icebergs
\end{frame}
%=============================================================


%====== #18 ==================================================
\begin{frame}{}

  \includegraphics[width=.9\textwidth]{images/ESA-AI-applications.png}
    
\end{frame}
%=============================================================


%====== #18 ==================================================
\begin{frame}{Applications \& Challenges of AI in Space Missions}

  Nowadays ML applications provides tools for all but few space missions...
  \medskip

  \hspace*{-2mm} %
  \begin{minipage}{.33\textwidth}
    \includegraphics[width=1.\textwidth]{images/space-ML.jpg}\\[-6mm]
                    \center{\tiny \href{https://phys.org/news/2021-06-spacemlorg-resource-ai-application-space.html}
                      {SpaceML: a machine learning toolbox\\[-2mm]  and developer community}}
  \end{minipage} %
  \begin{minipage}{.33\textwidth}
    \includegraphics[width=1.\textwidth]{images/earth-illustration-computer-graphics-overlay.png}\\[-6mm]
                    \center{\tiny \href{https://eos.org/science-updates/advancing-ai-for-earth-science-a-data-systems-perspective}
                      {Advancing AI for Earth Science:\\[-2mm] A Data Systems Perspective}}
  \end{minipage} %
  \begin{minipage}{.33\textwidth}
    \includegraphics[width=1.\textwidth, height=.58\textwidth]{images/Heliophysics-Featured-Image-1.png}\\[-6mm]
                    \center{\tiny \href{https://eos.org/editors-vox/machine-learning-helps-to-solve-problems-in-heliophysics}
                      {Machine Learning Helps to Solve\\[-2mm] Problems in Heliophysics}}
  \end{minipage}
  \medskip
  
  \hspace*{-2mm} %
  \begin{minipage}{.33\textwidth}
    \includegraphics[width=1.\textwidth]{images/ML-Featured-IMage-Final.png}\\[-6mm]
                    \center{\tiny \href{https://eos.org/editors-vox/jgr-machine-learning-and-computation-is-open-for-submissions}
                      {JGR (Journal of Geophysical Research):\\[-2mm] ML and Computation is Open for Submissions}}
  \end{minipage} %
  \begin{minipage}{.33\textwidth}
    \includegraphics[width=1.\textwidth]{images/earth-illustration-computer-graphics-overlay.png}\\[-6mm]
                    \center{\tiny \href{https://eos.org/science-updates/advancing-ai-for-earth-science-a-data-systems-perspective}
                      {Advancing AI for Earth Science:\\[-2mm] A Data Systems Perspective}}
  \end{minipage} %
  \begin{minipage}{.33\textwidth}
    \includegraphics[width=1.\textwidth, height=.58\textwidth]{images/Heliophysics-Featured-Image-1.png}\\[-6mm]
                    \center{\tiny \href{https://eos.org/editors-vox/machine-learning-helps-to-solve-problems-in-heliophysics}
                      {Machine Learning Helps to Solve\\[-2mm] Problems in Heliophysics}}
  \end{minipage}
  

\end{frame}
%=============================================================


\section{Computing aspects}

\subsection{The artificial neuron}

%====== #19 ==================================================
\begin{frame}{Computing aspects}
  
  \tikzset{%
  neuron/.style={
    circle,
    draw,
    minimum size=1cm,
    font=\large
  },
  squa/.style={
    draw,
    inner sep=2pt,
    font=\large,
    join = by -latex
  },
  }
  \begin{tcolorbox}[title=The Artificial neuron model]  
    %\hspace*{-.7cm}
    \begin{tikzpicture}[x=1.4cm, y=1.cm]

      \node [label=above:\parbox{2cm}{\centering Input\\stimuli}] at (0, 1.5) (x1)  {$x_1$};
      \node [] at (0, 0.5) (x2) {$x_2$};
      \node [] at (0, -0) (vdots) {$\vdots$};
      \node [] at (0, -0.7) (xn) {$x_n$};
      \node [label=above:\parbox{2cm}{\centering Bias}] at (2, 2) (bias) {$b$};
      \node [label=above:\parbox{2cm}{\centering Output}] at (4, 0.15) (y) {$y = f(\sum_i{w_{i}\,x_i} - b)$};
      
      \node [neuron/.try] (output) at (2,0.15) {\large{$\displaystyle{\Sigma | f}$}};
      
      \draw [o-latex] (x1) -- (output);
      \draw [o-latex] (x2) -- (output);
      \draw [o-latex] (xn) -- (output);
      \draw [o-latex] (bias) -- (output);
      \draw [->] (output) -- (y);

      \node [] at (1,1) () {$w_1$} ;
      \node [] at (1,.5) () {$w_2$} ;
      \node [] at (1, -0.45) () {$w_n$} ;
    \end{tikzpicture}
  \end{tcolorbox}
  \smallskip
  \visible<1->{An \bfdarkchoco{artificial neuron}:
    \begin{itemize}
    \item <1-> receives the input stimuli $(x_{i})_{i=1..n}$ with \textbf{weights} $(w_i)$
    \item <1-> computes the \textbf{weighted sum} of the input $\sum_i{w_{i}\,x_i - b}$
    \item <1-> outputs its activation $f(\sum_i{w_{i}\,x_i} - b)$, computed with a non-linear \textbf{activation function} $f$ .
    \end{itemize}
  }

\end{frame}
%=============================================================

\subsection{Activation functions}

%====== #20 ==================================================
\begin{frame}{Computing aspects}
  
  \begin{tcolorbox}[add to width=.7cm, title={Common activation functions}]
    \includegraphics[width=1.\textwidth]{images/activ_functions-2.png}
  \end{tcolorbox}
  
  \begin{itemize}
  \item Introduces a non-linear behavior.
  \item Sets the range of the neuron output: $[-1, 1]$, $[0, 1]$, $[0, \infty[$...
    \item The bias $b$ sets the activation threshold of the neuron.
  \end{itemize}
  
\end{frame}
%=============================================================

\subsection{Activation functions SoftMax}

%====== #21 ==================================================
\begin{frame}{Computing aspects}
  \vspace*{-2mm}

  \begin{itemize}
    
  \item intermediate layers $\leadsto$ \bfdarkchoco{relu} promotes network learning
    \footnote{{\tiny avoids the {\em vanishing gradient} that appears in {\em back propagation}}}.\smallskip
    
  \item \bfdarkchoco{softmax} always used for in the last layer for {\bf Classifying}.
    
  \end{itemize}
  \smallskip
  
  \begin{tcolorbox}[add to width=.7cm, title={Example: ativation function {\em softmax} for the last layer, 10 classes}]
    
    \begin{minipage}{.55\textwidth}
      \vspace*{-2mm}\includegraphics[width=0.8\textwidth]{images/softmax-2.png}
    \end{minipage}\hspace*{5mm}\begin{minipage}{.45\textwidth}
      {\small
        \begin{itemize}
        \item The activation of neuron $k$ is $Y_k = e^{y_k}/\sum_i{e^{y_i}}$
          with $y_k = \sum_i \omega_i x_i - b$ calculated by the neuron $k$.
        \item The neurons outputs are interpreted as {\bf probabilities} in the interval [0,1].
        \item[\Black{$\leadsto$}] <2> the label of the neuron with the highest probability is the network response
      \end{itemize}}
    \end{minipage}
  \end{tcolorbox}
    
\end{frame}
%=============================================================

\subsection{One-hot coding}

%====== #22 ==================================================
\begin{frame}{Computing aspects [Classification]}

\begin{tcolorbox}[title={\em One-hot} coding of labels]

     Goal: transform the image labels to match the network output (vector of probabiliies):

     {\small
       \begin{itemize}
       \item Image labels: ordered set \textbf{integers}.
       \item Network output: \textbf{vector of \texttt{float}} in the interval [0;1] calculated by the {\em softmax} activation of the output neurons.
       \item {\em One-hot} coding of an ordered set $\mathfrak{L}$ of $N$ labels: \\[1mm]
         - each label value is coded as a vector with $N$ components all zero except one (equal to $1$),\\
         - the rank of the $1$ in the vector gives the value of the labe.
       \end{itemize}
     }
\end{tcolorbox}

\visible<2>{
  \begin{minipage}{.3\textwidth}
    \vspace*{-22mm}\hspace*{-5mm}\includegraphics[width=1.1\textwidth]{images/oneHotCoded.png}
  \end{minipage}
  \begin{minipage}{.6\textwidth}
    \vspace*{-5mm}{\small The {\em one-hot} encoding of the labels '0' to '9' gives a vector with 10 components.}
  \end{minipage}
}
    
\end{frame}
%=============================================================

%====== #23 ==================================================
\begin{frame}{Computing aspects [Classification]}

\begin{tcolorbox}[title=Error function: {\em Cross entropy error}]

  \begin{itemize}
  \item The image analysed by the network $\leadsto$ vector $\hat{Y}$ of \texttt{float} to be compared to the vector $Y$ of the {\em hot-one} encoding of the true image label.
  \item The error (loss) function {\em cross entropy} is adapted to {\em one-hot} coding: $e(Y,\hat{Y})=-\sum_i Y_i\ log(\hat {Y}_i)$\\
    \includegraphics[width=.9\textwidth]{images/CrossEntropyError.png}
  \end{itemize}
  
   \end{tcolorbox}  
\end{frame}
%=============================================================

\subsection{Optimisation}

%====== #24 ==================================================
\begin{frame}{Computing aspects}

\begin{tcolorbox}[title=Optimization and {\em Back Propagation}]

  \begin{itemize}
    \visible<1->{\item During the training an optimization algorithm computes the gradient of the error function
      with respect to the network weights.}
    \visible<2->{\item The {\em Back Propagation} algorithm \textbf{modifies} the weights of the network layer by layer thanks to the gradient of the error function,
      iterating from the last layer to the first layer. }
    \visible<3->{\item Examples of optimization algorithms:
      \begin{itemize}
      \item Gradient Descent ({\em Gradient Descent (GD)})
      \item Stochastic Gradient Descent ({\em Stochastic Gradient Descent (SGD)})
      \item {\em \href{https://arxiv.org/abs/1412.6980}{Adam}} (improved version of gradient descent)...
      \end{itemize}

      {\small The module \href{https://www.tensorflow.org/api_docs/python/tf/keras/optimizers}{tf.keras.optimizers}
        provides Python implementation of several optimization algorithms.}}
      
  \end{itemize}
\end{tcolorbox}

\end{frame}
%=============================================================

\subsection{Back-propgation algorithm}

%====== #25 ==================================================
\begin{frame}{Computing aspects}

  {\small Visualization of iterations of gradient descent algorithms for an very-simple loss function with only 2 variables:}\\[2mm]
  \hspace*{25mm}\href{https://github.com/Jaewan-Yun/optimizer-visualization/blob/master/figures/movie9.gif}{\includegraphics[width=.45\textwidth]{images/adam_plot3D_animated.png}}\\[-2mm]
  %\hspace*{8mm}\includegraphics[width=.8\textwidth]{images/adam_plot3D_animated.png}\\[-2mm]
  %\animategraphics[width=.8\textwidth,controls]{10}{./images/Adam-plot3D/movie9/img_}{0}{114}
  \hspace*{25mm}{\tiny (source: \href{https://github.com/Jaewan-Yun/optimizer-visualization}{github.com/Jaewan-Yun/optimizer-visualization})}
  
  \medskip
      {\small Video explaining the {\em back propagation} algorithm:}\\
      \hspace*{30mm}\href{https://www.3blue1brown.com/lessons/backpropagation}{\includegraphics[width=.35\textwidth]{images/video-BackPropagation.png}}
\end{frame}
%=============================================================

\subsection{Supervised learning}

%====== #26 ==================================================
\begin{frame}{Computing aspects}

  {\small
    \vspace*{-1mm}\hspace*{-10mm}\includegraphics[width=1.18\textwidth]{./images/NetworkTraining.png}
  \vspace*{-4mm}\begin{itemize}
  \item The dataset is split into (mini) {\bf batches} of size \code{batch\_size}
  \item After each {\em feed forward} the {\em Back Propagation} algorithm modifies the weights
    neurons to minimize the error $e$.
  \end{itemize}
  }
\end{frame}
%=============================================================

\subsection{Supervised learning}

%====== #27 ==================================================
\begin{frame}{Computing aspects}

  \includegraphics[width=\textwidth]{./images/NetworkTraining_2.png}
  \vspace*{-5mm}\begin{itemize}
  \item Training with the whole dataset is repeated \code{n\_epoch} times,
  \item The network state at the end of epoch \code{n} becomes the initial state for epoch \code{n+1}.
   \end{itemize}  
\end{frame}
%=============================================================

\end{document}


\section{References}

%=============================================================
\begin{frame}{Videos}

  \hspace*{-3mm}
  \href{https://youtu.be/trWrEWfhTVg}{\includegraphics[width=.5\textwidth]{images/video-LeDeepLearning.png}}
  \hfill%
  \href{https://www.3blue1brown.com/lessons/neural-networks}{\includegraphics[width=.5\textwidth]{images/video-NeuralNetworks.png}}\\[-2mm]

  {\tiny\hspace*{-2mm}\href{run:./videos/Le deep learning - YouTube.webm}{1/ Local: "Le deep learning - YouTube.webm"}%
    \hfill%
    \href{run:./videos/But what is a neural network.webm}{2/ local : "But what is a neural network.webm"}}\\[2mm]
            
  \hspace*{-3mm}
  \href{https://www.3blue1brown.com/lessons/gradient-descent}{\includegraphics[width=.5\textwidth]{images/video-HowMachineLearn.png}}
  \hfill
  \href{https://www.3blue1brown.com/lessons/backpropagation}{\includegraphics[width=.5\textwidth]{images/video-BackPropagation.png}}\\[-2mm]
  
  {\tiny\hspace*{-3mm}\href{run:./videos/Gradient descent how neural networks learn.webm}{3/ Local: "Gradient descent how neural networks learn.webm"}%
    \hfill%
    \href{run:./videos/What is backpropagation really doing .webm}{4/ Local: "What is backpropagation really doing .webm"}}\\[2mm]
  
\end{frame}
%=============================================================

%=============================================================
\begin{frame}{References}
  
  \noindent\fontsize{8}{8}\selectfont{

    \hspace*{-4mm}\hypertarget{refRusselNorvig}{[1] }%
    {\em Artificial Intelligence: A Modern Approach (4th Edition)}, By Stuart Russell \& Peter Norvig. 
    Pearson, 2020. ISBN 978-0134610993. \href{http://aima.cs.berkeley.edu/}{aima.cs.berkeley.edu}\\[3mm]
    {\em Intelligence artificielle -- Une approche moderne -- 4e éd.}, By Stuart Russell \& Peter Norvig. 
    Translated by L. Miclet, F. Popineau, \& C. Cadet. Paris: Pearson Education France, 2021. ISBN 978-2326002210.\\[5mm]


    \hspace*{-4mm}\hypertarget{refStrongWeak-AI}{[2] }%
    {\em What is artificial intelligence (AI), and what is the difference between general AI and narrow AI?}, Kris Hammond, 2015\\
    \href{https://www.computerworld.com/article/2906336/what-is-artificial-intelligence.html}
         {www.computerworld.com/article/2906336/what-is-artificial-intelligence.html}\\[5mm]

    \hspace*{-4mm}\hypertarget{StanfordEncyc}{[3] }%
    {\em Stanford Encyclopedia of Philosophy},
    \href{https://plato.stanford.edu/entries/artificial-intelligence}
         {plato.stanford.edu/entries/artificial-intelligence}\\[5mm]

    \hspace*{-4mm}\hypertarget{DeepLeraning}{[4] }%
    {\em  Deep Learning.}, Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016), MIT Pres, ISBN 9780262035613
  }
  
\end{frame}

%=============================================================

\end{document}

\begin{frame}{Biliographiesymbols for figures}

  \begin{figure}[hb]
    \begin{center}
        \input{images/NetworkTraining-export.tex}
    \end{center}
    \caption[]{\label{fig:latex_dia_integration} A sample font-consistent figure
    }
  \end{figure}
  
\end{frame}
%=============================================================

[ML-01] – JLC  
« Machine Learning for Humans » 
https://medium.com/machine-learning-for-humans

Point d'entrée du site « Machine Learning for Humans », à partir duquel on peut aller sur "A Beginner's Guide to AI/ML" et d'autres documents très intéressants.


[ML-02] – JLC  
« A Beginner’s Guide to AI/ML »
https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12

Cours complet : supervised learning, unsupervised learning, Neural networks & deep learning, reinforcement learning....Version PDF du cours également disponible. sur : https://www.dropbox.com/s/e38nil1dnl7481q/machine_learning.pdf?dl=0

[ML-03] – VG
« ML Basics : Unsupervised, supervised and reinforcement learning »
Article court mais utile pour bien saisir la différence entre supervised, unsupervised et reinforcement learning.
https://medium.com/@machadogj/ml-basics-supervised-unsupervised-and-reinforcement-learning-b18108487c5a

DRL  (Deep Reinforcement Learning)
[DRL-01] – JLC 
« Introduction to Various Reinforcement Learning Algorithms. Part I (Q-Learning, SARSA, DQN, DDPG) »
https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287

[DRL-02] – VG
https://deepmind.com/blog/deep-reinforcement-learning/
Récapitulatif de ce qu’on sait faire aujourd’hui en DRL fait par DeepMind (un des leaders mondiaux en DRL, concepteurs d’AlphaGo notamment) 

[DRL-03] – JLC
« Demystifying Deeep Reinforcement Learning », Tambet Matiisen
https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/
Article très clair et très pédagogique sur ML, RL et DQN.



\footnote{\tiny \hyperlink{DeepLeraning}{[2]} {\em Deep Learning.}, Goodfellow {\em et al.}, Chapitre {\em "6.5 Back-Propagation and Other Differentiation Algorithms"}}

IBM Watson https://www.ibm.com/watson/products-services


*******************
*******************

    one episode = one a sequence of states, actions and rewards, which ends with terminal state. For example, playing an entire game can be considered as one episode, the terminal state being reached when one player loses/wins/draws. Sometime, one may prefer to define one episode as several games (example: "each episode is a few dozen games, because the games go up to score of 21 for either player").
    one epoch = one forward pass and one backward pass of all the training examples, in the neural network terminology.

    \subsection{Neural Networks achitectures}

%=============================================================
\begin{frame}{Neural Network Architectures}

  Examples of common NN architectures:
\begin{itemize}
  \item \bfdarkchoco{Dense} Sequential (DNN): the simplest NN made of successive layers of neurones, with {\em Feed Forward} and {\em Back Propagation} algorithms.
  \item \bfdarkchoco{Convolutional} (CNN): Mostly used for analyzing and classifying images.
  \item \bfdarkchoco{Recurrent} (RNN): Used to learn from time series, like the Long short-term memory (LSTM) algorithm.
  \item \bfdarkchoco{Generative adversarial} (GAN): can generate images, text...
\end{itemize}
\end{frame}
%=============================================================

%=============================================================
\begin{frame}
  \begin{tcolorbox}[title={RL ingredients: \textbf{Policy}, \textbf{Value function}}, fonttitle=\Large]    
    \begin{itemize}
    \item <1-> The \bfdarkchoco{Policy} $\pi(a|s)$ is a probalistic mapping between action $a$ and state $s$.
      \begin{itemize}
      \item <2-> can be as simple as a look-up table (Q-learning) or involve extensive computation.
      \end{itemize}
      \item Is the core of a DRL in the sense that it alone is sufficient to determine behavior.
    \item <3-> The \bfdarkchoco{Value function} selects actions that bring states of the highest value over the long run.
    \end{itemize}    
  \end{tcolorbox}
\end{frame}
%=============================================================
